import csv
import re
import numpy as np
import pandas as pd
import json
import os
import random
import sys
import argparse

from collections import defaultdict, Counter


class HigherOrderMarkovModel:
    """
    A higher-order Markov model for generating words based on training data.

    Attributes:
        order (int): The order of the Markov model.
        transition_probs (defaultdict): A dictionary storing transition probabilities for each context.
    """
    def __init__(self, order):
        """
        Initializes the HigherOrderMarkovModel with a specified order.

        Args:
            order (int): The order of the Markov model.
        """
        self.order = order
        self.transition_probs = defaultdict(Counter)

    def train(self, words):
        """
        Trains the Markov model on a list of words.

        Args:
            words (list): A list of words to train the model on.

        Returns:
            None
        """
        for word in words:
            padded_word = f"{' ' * self.order}{word}{' ' * self.order}"
            for i in range(len(padded_word) - self.order):
                context = padded_word[i:i + self.order]
                next_char = padded_word[i + self.order]
                self.transition_probs[context][next_char] += 1

        # Convert counts to probabilities
        for context, counter in self.transition_probs.items():
            total = float(sum(counter.values()))
            for next_char in counter:
                self.transition_probs[context][next_char] /= total

    def generate(self, num_words, max_length):
        """
        Generates a specified number of words using the trained Markov model.

        Args:
            num_words (int): The number of words to generate.
            max_length (int): The maximum length of each generated word.

        Returns:
            list: A list of generated words.
        """
        words = []
        for _ in range(num_words):
            context = ' ' * self.order
            word = []
            for _ in range(max_length):
                if context not in self.transition_probs:
                    break
                next_char = random.choices(
                    list(self.transition_probs[context].keys()),
                    list(self.transition_probs[context].values())
                )[0]
                if next_char == ' ':
                    break
                word.append(next_char)
                context = (context + next_char)[1:]
            words.append(''.join(word))
        return words


def get_phoneme_set(phonotactics_file):
    """
    Reads a TSV file and returns two sets of phonemes: all unique phonemes and unique vowels.

    Args:
        phonotactics_file (str): Path to the TSV file containing phonotactics data.

    Returns:
        tuple: A tuple containing two lists:
            - phoneme_set (list): A sorted list of all unique phonemes.
            - vowels (list): A sorted list of unique vowels, case insensitive and ignoring diacritics.
    """
    df = pd.read_csv(phonotactics_file, sep='\t')
    phoneme_set = sorted(list(set(cell for cell in df.values.flatten() if isinstance(cell, str))))
    vowel_regex = re.compile(r'[aeiou]', re.IGNORECASE)
    vowels = sorted(list(set(cell for cell in df.values.flatten() if isinstance(cell, str) and vowel_regex.search(str(cell)))))
    return phoneme_set, vowels


def read_input_file(inputfile):
    """
    Reads an input file and returns a list of non-empty, stripped lines.

    Args:
        inputfile (str): Path to the input file.

    Returns:
        list: A list of non-empty, stripped lines from the input file.
    """
    with open(inputfile, "r") as f:
        return [line.strip() for line in f if line.strip()]


def create_and_train_model(words, model_order):
    """
    Creates and trains a HigherOrderMarkovModel with the given words and model order.

    Args:
        words (list): A list of words to train the model on.
        model_order (int): The order of the Markov model.

    Returns:
        HigherOrderMarkovModel: The trained Markov model.
    """
    markov_model = HigherOrderMarkovModel(model_order)
    markov_model.train(words)
    return markov_model


def process_markov_output(words, phonotactics_file, phoneme_set, vowels):
    """
    Processes Markov model output words by applying phonotactic constraints and removing invalid phonemes.

    Args:
        words (list): A list of words generated by the Markov model.
        phonotactics_file (str): Path to the TSV file containing phonotactic constraints.
        phoneme_set (list): A list of all unique phonemes.
        vowels (list): A list of unique vowels.

    Returns:
        list: A list of processed words that adhere to phonotactic constraints.
    """
    phonotactics = load_phonotactics(phonotactics_file)
    permitted_onsets = set(phonotactics['onset'])
    permitted_codas = set(phonotactics['coda'])
    permitted_nuclei = set(phonotactics['nucleus'])
    output_words = []
    for word in words:
        processed_word = remove_invalid_phonemes(word, phoneme_set)
        applied_phonotactics = apply_phonotactics(processed_word, permitted_onsets, permitted_nuclei, permitted_codas, phoneme_set, vowels)
        output_words.append(applied_phonotactics)
    return output_words


def write_file(wordlist, outfile):
    """
    Writes a list of words to an output file.

    Args:
        wordlist (list): List of words to write to the file.
        outfile (str): Path to the output file.
    """
    with open(outfile, "w") as f:
        for word in wordlist:
            f.write(word + '\n')


def post_process(patterns, infile, outfile):
    """
    Post-processes a text file by applying regex patterns and removing duplicate lines.

    Args:
        patterns (str): Path to the JSON file containing regex patterns.
        infile (str): Path to the input file to be post-processed.
        outfile (str): Path to the output file to write the processed text.
    """
    # Load patterns from file
    with open(patterns, "r") as patf:
        pats = json.load(patf)
    
    # Process input file and write to output file
    with open(infile, "r") as f1:
        seen_lines = set()
        with open(outfile, "w") as f2:
            for line in f1:
                if line not in seen_lines:
                    seen_lines.add(line)
                    for pattern, replacement in pats.items():
                        line = re.sub(re.compile(pattern), replacement, line)
                    f2.write(line)


def load_phonotactics(phonotactics_file):
    """
    Loads phonotactic constraints from a TSV file and returns a dictionary with onsets, nuclei, and codas.

    Args:
        phonotactics_file (str): Path to the TSV file containing phonotactic constraints.

    Returns:
        dict: A dictionary with keys 'onset', 'nucleus', and 'coda', each containing a list of phonotactic constraints.
    """
    phonotactics = {}
    onset = []
    nucleus = []
    coda = []
    with open(phonotactics_file, 'r', newline='') as tsvfile:
        reader = csv.reader(tsvfile, delimiter='\t')
        next(reader) # Skip header
        for row in reader:
            if row[0]:
                onset.append(row[0])
            if row[1]:
                nucleus.append(row[1])
            if row[2]:
                coda.append(row[2])
    phonotactics['onset'] = onset
    phonotactics['nucleus'] = nucleus
    phonotactics['coda'] = coda
    return phonotactics


def split_to_phonemes(word, phoneme_set):
    """
    Splits a word into its constituent phonemes based on a given set of phonemes.

    Args:
        word (str): The word to be split.
        phoneme_set (set): A set of phonemes to use for splitting the word.

    Returns:
        list: A list of phonemes that make up the word.
    """
    phonemes = []
    i = 0
    while i < len(word):
        found_phoneme = False
        for phoneme in sorted(phoneme_set, key=len, reverse=True):
            if word[i:i + len(phoneme)] == phoneme:
                phonemes.append(phoneme)
                i += len(phoneme)
                found_phoneme = True
                break
        if not found_phoneme:
            # If no matching phoneme is found, treat the character as a single phoneme.
            phonemes.append(word[i])
            i += 1
    return phonemes


def remove_invalid_phonemes(word, phoneme_set):
    """
    Removes invalid phonemes from a word based on a given set of phonemes.

    Args:
        word (str): The word to process.
        phoneme_set (set): A set of valid phonemes.

    Returns:
        str: The word with invalid phonemes removed.
    """
    phonemes = split_to_phonemes(word, phoneme_set)
    output_phonemes = []
    for ph in phonemes:
        if ph in phoneme_set:
            output_phonemes.append(ph)
    return "".join(output_phonemes)


def next_vowel(word, index, vowels):
    """
    Finds the next vowel in a word starting from a given index.

    Args:
        word (str): The word to search.
        index (int): The starting index for the search.
        vowels (list): A list of vowel characters.

    Returns:
        str: The next vowel found in the word, or 'a' if no vowel is found.
    """
    for i in range(index, len(word)):
        if word[i] in vowels:
            return word[i]
    return 'a'


def previous_vowel(word, index, vowels):
    """
    Finds the previous vowel in a word starting from a given index.

    Args:
        word (str): The word to search.
        index (int): The starting index for the search, moving backwards.
        vowels (list): A list of vowel characters.

    Returns:
        str: The previous vowel found in the word, or 'a' if no vowel is found.
    """
    for i in range(index, -1, -1):
        if word[i] in vowels:
            return word[i]
    return 'a'


def has_no_vowels(word, vowels):
    """
    Checks if a word contains no vowels.

    Args:
        word (str): The word to check.
        vowels (list): A list of vowel characters.

    Returns:
        bool: True if the word contains no vowels, False otherwise.
    """
    for ph in word:
        if ph in vowels:
            return False
    return True


def generate_word_structure(phonemes, vowels):
    """
    Generates a representation of the structure of a word, including onsets, nuclei, medials and coda.

    Args:
        phonemes (list): A list of phonemes that make up the word.
        vowels (list): A list of vowel characters.

    Returns:
        list: A list of dictionaries representing the structure of the word, including onsets, nuclei, medials, and coda.
    """
    word_structure = {
        "onset": None,
        "nuclei": [],
        "medials": [],
        "coda": None
    }
    # Get the positions of all the vowels in the phonemes
    vowel_indices = [i for i, ph in enumerate(phonemes) if ph in vowels]
    if not vowel_indices:
        # No vowels in the word; return just the phonemes as onset
        word_structure["onset"] = {"type": "onset", "segment": ''.join(phonemes), "start": 0, "end": len(phonemes)-1}
        return [word_structure["onset"]]
    # Identify onset
    if phonemes[0] not in vowels:
        word_structure["onset"] = {"type": "onset", "segment": ''.join(phonemes[:vowel_indices[0]]), "start": 0, "end": vowel_indices[0]-1}
    # Identify nuclei and medials
    curr_nucleus = {"start": vowel_indices[0], "end": vowel_indices[0]}
    for i in range(1, len(vowel_indices)):
        curr_vowel = vowel_indices[i]
        prev_vowel = vowel_indices[i-1]           
        if curr_vowel - prev_vowel == 1:  # extend current nucleus
            curr_nucleus["end"] = curr_vowel
        else:
            # Append nucleus
            nucleus_segment = ''.join(phonemes[curr_nucleus["start"]:curr_nucleus["end"]+1])
            word_structure["nuclei"].append({"type": "nucleus", "segment": nucleus_segment, "start": curr_nucleus["start"], "end": curr_nucleus["end"]})              
            # Append medial
            medial_segment = ''.join(phonemes[curr_nucleus["end"]+1:curr_vowel])
            word_structure["medials"].append({"type": "medial", "segment": medial_segment, "start": curr_nucleus["end"]+1, "end": curr_vowel-1})                
            # Start new nucleus
            curr_nucleus = {"start": curr_vowel, "end": curr_vowel}
    # Finalize the last nucleus after loop ends
    nucleus_segment = ''.join(phonemes[curr_nucleus["start"]:curr_nucleus["end"]+1])
    word_structure["nuclei"].append({"type": "nucleus", "segment": nucleus_segment, "start": curr_nucleus["start"], "end": curr_nucleus["end"]})
    # Identify coda
    if vowel_indices[-1] < len(phonemes)-1:
        word_structure["coda"] = {"type": "coda", "segment": ''.join(phonemes[vowel_indices[-1]+1:]), "start": vowel_indices[-1], "end": len(phonemes)-1}
    # Produce ordered list of word structure elements
    result = []
    if word_structure["onset"]: 
        result += [word_structure["onset"]]
    nuclei_and_medials = [item for pair in zip(word_structure["nuclei"], word_structure["medials"]) for item in pair] + word_structure["nuclei"][len(word_structure["medials"]):] + word_structure["medials"][len(word_structure["nuclei"]):]
    result += nuclei_and_medials
    if word_structure["coda"]:
        result += [word_structure["coda"]]
    return result


def isViolation(struct, permitted_onsets, permitted_nuclei, permitted_codas):
    """
    Checks if a given word structure element violates phonotactic constraints.

    Args:
        struct (dict): A dictionary representing a word structure element.
        permitted_onsets (set): A set of permitted onsets.
        permitted_nuclei (set): A set of permitted nuclei.
        permitted_codas (set): A set of permitted codas.

    Returns:
        bool: True if the structure element violates phonotactic constraints, False otherwise.
    """
    if struct["type"] == "onset":
        if struct["segment"] in permitted_onsets:
            return False
    elif struct["type"] == "nucleus":
        # Deal with vowel hiatus violations using patterns json
        return False
    elif struct["type"] == "medial":
        if struct["segment"] in permitted_onsets or len(struct["segment"]) == 1:
            return False
        elif len(struct["segment"]) == 2 and struct["segment"] in permitted_codas:
            return False
        elif len(struct["segment"]) == 2 and struct["segment"][0] in permitted_codas and struct["segment"][1] in permitted_onsets:
            return False
        elif len(struct["segment"]) == 3 and struct["segment"][0] in permitted_codas and struct["segment"][1:] in permitted_onsets:
            return False
        elif len(struct["segment"]) == 3 and struct["segment"][:2] in permitted_codas and struct["segment"][2] in permitted_onsets:
            return False
        elif len(struct["segment"]) == 4 and struct["segment"][:2] in permitted_codas and struct["segment"][2:] in permitted_onsets:
            return False
    elif struct["type"] == "coda":
        if struct["segment"] in permitted_codas:
            return False
    return True


def fix_violation(struct, phonemes, permitted_onsets, permitted_nuclei, permitted_codas, vowels):
    """
    Fixes phonotactic violations in a word structure element.

    Args:
        struct (dict): A dictionary representing a word structure element.
        phonemes (list): A list of phonemes that make up the word.
        permitted_onsets (set): A set of permitted onsets.
        permitted_nuclei (set): A set of permitted nuclei.
        permitted_codas (set): A set of permitted codas.
        vowels (list): A list of vowel characters.

    Returns:
        list: A list of phonemes with the violation fixed.
    """
    if struct["type"] == "onset":
        if struct["segment"].startswith('s'):
            phonemes.insert(0, 'i')
        elif has_no_vowels(''.join(phonemes), vowels):
            insertion_position = len(phonemes) // 2
            phonemes.insert(insertion_position, 'a')
        else:
            insertion_position = struct["start"] + 1
            phonemes.insert(insertion_position, next_vowel(phonemes, insertion_position, vowels))
    elif struct["type"] == "medial":
        if len(struct["segment"]) >= 3:
            # Handle potential CC coda
            if struct["segment"][:2] in permitted_codas and struct["segment"][2] in permitted_onsets:
                insertion_position = struct["start"] + 2
                phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
            else:
                # Split into potential coda C and onset CC
                coda_candidate = struct["segment"][0]
                onset_candidate = struct["segment"][1:3]
                # Check if split results in valid coda and onset
                if coda_candidate in permitted_codas and onset_candidate in permitted_onsets:
                    insertion_position = struct["start"] + 2
                    phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
                else:
                    if struct["segment"][:2] in permitted_onsets or (struct["segment"][0] in permitted_codas and struct["segment"][1] in permitted_onsets):
                        insertion_position = struct["start"] + 1
                        phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
                    else:
                        insertion_position = struct["start"] + 1
                        phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
        # Handle two consonants
        else:
            insertion_position = struct["start"] + 1
            phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
    else:  #coda
        if len(struct["segment"]) > 3:
            insertion_position = struct["start"] + 2
            phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
        elif len(struct["segment"]) == 3:
            # Handle potential CC coda
            if struct["segment"][:2] in permitted_codas and struct["segment"][2] in permitted_onsets:
                insertion_position = struct["start"] + 2
                phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
            else:
                if struct["segment"][0] in permitted_codas:
                    if struct["segment"][1:] in permitted_onsets:
                        phonemes.append(previous_vowel(phonemes, len(phonemes) - 1, vowels))
                    else:  # second & third consonants not in permitted onsets
                        insertion_position = struct["start"] + 2
                        phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
                else:  # first consonant not in permitted codas
                    insertion_position = struct["start"] + 2
                    phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
        elif len(struct["segment"]) == 2:
            if struct["segment"][0] in permitted_codas or struct["segment"] in permitted_onsets:
                phonemes.append(previous_vowel(phonemes, len(phonemes) - 1, vowels))
            else:  # first consonant not in permitted codas
                insertion_position = struct["start"] + 2
                phonemes.insert(insertion_position, previous_vowel(phonemes, insertion_position, vowels))
        else:  # one consonant
            phonemes.append(previous_vowel(phonemes, len(phonemes) - 1, vowels))
    return phonemes


def apply_phonotactics(word, permitted_onsets, permitted_nuclei, permitted_codas, phoneme_set, vowels):
    """
    Applies phonotactic constraints to a word.

    Args:
        word (str): The word to process.
        permitted_onsets (set): A set of permitted onsets.
        permitted_nuclei (set): A set of permitted nuclei.
        permitted_codas (set): A set of permitted codas.
        phoneme_set (list): A list of all unique phonemes.
        vowels (list): A list of vowel characters.

    Returns:
        str: The word with phonotactic constraints applied.
    """
    phonemes = split_to_phonemes(word, phoneme_set)
    word_structure = generate_word_structure(phonemes, vowels)
    i = 0
    while i < len(word_structure):
        if isViolation(word_structure[i], permitted_onsets, permitted_nuclei, permitted_codas):
            phonemes = fix_violation(word_structure[i], phonemes, permitted_onsets, permitted_nuclei, permitted_codas, vowels)
            word_structure = generate_word_structure(phonemes, vowels)
            i = 0
        else:
            i += 1
    return "".join(phonemes)


def post_process(patterns, infile, outfile):
    """
    Post-processes a text file by applying regex patterns and removing duplicate lines.

    Args:
        patterns (str): Path to the JSON file containing regex patterns.
        infile (str): Path to the input file to be post-processed.
        outfile (str): Path to the output file to write the processed text.
    """
    # Load patterns from file
    with open(patterns, "r") as patf:
        pats = json.load(patf)
    
    # Process input file and write to output file
    with open(infile, "r") as f1:
        seen_lines = set()
        with open(outfile, "w") as f2:
            for line in f1:
                if line not in seen_lines:
                    seen_lines.add(line)
                    for pattern, replacement in pats.items():
                        line = re.sub(re.compile(pattern, re.MULTILINE), replacement, line)
                    f2.write(line)


def clean_up():
    """
    Removes intermediate output files if they exist.
    """
    intermediate_files = ["output.txt"]
    for file in intermediate_files:
        if os.path.exists(file):
            os.remove(file)


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-i", "--inputfile", help="Input file with wordlist for training Markov model.", default="data/swadesh.txt")
    parser.add_argument("-m", "--model_order", help="Order of Markov model (preceding character window).", type=int, default=3)
    parser.add_argument("-l", "--max_length", help="Maximum length of input sequence.", type=int, default=10)
    parser.add_argument("-o", "--outputlines", help="Number of output words generated.", type=int, default=3000)
    parser.add_argument("-t", "--phonotactics_file", help="TSV file containing phonotactic rules.", default="data/phonotactics.tsv")
    parser.add_argument("-p", "--patterns", help="Optional json file for post-processing rules.", action='store_true', default=False)
    return parser.parse_args()


def main():
    args = get_args()

    phoneme_set, vowels = get_phoneme_set(args.phonotactics_file)
    words = read_input_file(args.inputfile)
    markov_model = create_and_train_model(words, args.model_order)
    generated_words = markov_model.generate(args.outputlines, args.max_length)
    processed_words = process_markov_output(generated_words, args.phonotactics_file, phoneme_set, vowels)
    for word in processed_words:
        print(word)
    write_file(processed_words, "output.txt")

    markov_fname = f"markov_{args.model_order}_output.txt"
    if args.patterns:
        patterns_file = input("Enter filepath for patterns: (default=data/markov_patterns.json) ") or "data/markov_patterns.json"
        post_process(patterns_file, "output.txt", markov_fname)
        clean_up()
    else:
        os.rename("output.txt", markov_fname)
        clean_up()

if __name__ == '__main__':
    main()